#lang ivy1.8

include collections

include messages
include multi_paxos
include utils
include collections

module manager_mod = {

    process manager(self: manager_id) = {


        var view : nat                      # The current view number
        var time : nat                      # The current time in seconds
        var heard(X:server_id) : nat        # Last time we head from server `X`
        var cur_servers : vector[server_id] # current servers in view
        var all_servers : vector[server_id] # all available servers
        var proposed : bool                 # Have we proposed a new view?

        common {
            parameter fail_time : nat = 2
            instance opt_view_msg : option(view_msg)
        }

        instance sock : man_net.socket
        instance server_sock : net.socket
        instance timer : timeout_sec
        instance paxos : multi_paxos(manager_id, opt_view_msg, opt_view_msg.empty) 


        implementation {

            after init {
                view := 0;
                time := 0;

                proposed := false;
                cur_servers := cur_servers.append(0);
                cur_servers := cur_servers.append(1);
                cur_servers := cur_servers.append(2);

                var i : nat := 0;
                while i <= cast(server_id.max) {
                    all_servers := all_servers.append(cast(i));
                    i := i + 1;
                }

            }

            # just here in case we want to add heartbeats + timeouts
            function is_up(S:server_id) = time <= heard(S) + fail_time

            # When the failure detector detects a node has parted
            export action is_down(s_down:server_id)

            implement sock.recv(src:tcp.endpoint,msg:man_msg_t) {
                msg.handle(self);
            }

            # pack up new servers 
            implement is_down {
                var new_servers : vector[server_id];
                for it, sv in cur_servers {
                    if sv ~= s_down {
                        new_servers := new_servers.append(sv);
                    }
                }
                # only send out new view if we haven't already deleted the server
                if new_servers ~= cur_servers {
                    announce(view.next, new_servers);
                }
            }

            action announce(v:nat, new_servers:vector[server_id]) = {
                if ~proposed {
                    var msg : view_msg;
                    msg.cur_servers := new_servers;
                    paxos.server.propose(manager.opt_view_msg.just(msg));
                    proposed := true;
                }
            }

            implement paxos.server.decide(inst: paxos.instance_t, op: opt_view_msg) {
                proposed := false;
                if ~op.is_empty {
                    view := view.next;
                    var msg : view_msg := op.contents;
                    cur_servers := msg.cur_servers;
                    msg.view := view;
                    broadcast(msg);
                }
            }

            action broadcast(msg:view_msg) = {
                for it, sv in all_servers  {
                    debug "sending new view" with server=sv, msg=msg;
                    server_sock.send(server(sv).sock.id, msg);
                }
            }

            implement join_msg_t.handle(msg: join_msg_t) {
                var dst := cur_servers.get(0); # an "arbitrary" node to replicate over

                debug "join_msg_t.handle" with self=self, msg=msg, dst=dst;

                var m2 : bootstrap_msg_t;
                m2.manager_src := self;
                m2.new_server := msg.src;

                server_sock.send(server(dst).sock.id, m2);
            }
        }

        specification {
            before is_down {
                require server(s_down).server_state = live;
            }

        }
    }

    # Topology changes should be very infrequent just so we spread the events out
    # through the test run.
    attribute manager.is_down.weight = "0.01"
}

