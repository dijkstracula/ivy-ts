#lang ivy1.8

# A tuplespace server makes up a part of the replicated system.  It can consume
# API actions from an embedded client or from other nodes.

include collections
include order
include network

include tablet
include ts_types

global {

    # All server messages are internal to the tuplespace implementation;
    # clients do not directly send these messages but simply "call into" the
    # tuplespace with exported actions.

    # TODO: It would be nice to be able to namespace these messages under
    # `server` to imply a certain information hiding, but I get errors about
    # dependency cycles...
    class msg_t = {
        # The operation to take when a node receives a particular message.
        action handle(self: node, ^msg:msg_t)

        # If the message is part of a larger protocol (such as a req/resp
        # pair), produce the message that should be returned to the sending
        # node.  Produces an empty return value if no such message need be
        # sent.  (The caller may still need to fill in certain fields.)
        # TODO: is this actually useful?  I'm not so sure anymore.
        # TODO: revisit the nathan/rpc_metaprotocol branch if there's time.
        # The `andThen` combinator might be what we want here.
        #action next(self: node, ^msg:msg_t) returns (ret: option[msg_t])
    }


    # A store is an internal operation mutates the tuplespace by adding a tuple
    # to all nodes in the tuplespace.  For details, see doc/operations.md .

    subclass store_req of msg_t = {
        field src : node
        field tpl : tuple

        action handle(self: node, ^msg: store_req)
    }

    subclass store_resp of msg_t = {
        field src : node
        field tpl : tuple
        field idem : bool # Was this an idempotent write (ie. did this tuple already exist?)

        action handle(self: node, ^msg: store_resp)
    }

    # A mark indicates that a remote server wishes to atomically remove a value
    # from the tuplestore.  This is phase one of the 2PC protocol.

    subclass mark_req of msg_t = {
        field src : node
        field tpl : tuple

        action handle(self: node, ^msg: mark_req)
    }

    subclass mark_resp of msg_t = {
        field src : node
        field tpl : tuple
        field ok : bool # Did we mark this successfully?

        action handle(self: node, ^msg: mark_resp)
    }


    # A mark rollback indicates that a remote server, having previously marked
    # some tuples during the first phase of a remove operation, would like to
    # undo those markings.

    subclass undo_mark_req of msg_t = {
        field src : node
        field tpl : tuple

        action handle(self: node, ^msg: undo_mark_req)
    }

    subclass undo_mark_resp of msg_t = {
        field src : node
        field tpl : tuple
        field ok : bool # Did we mark this successfully?

        action handle(self: node, ^msg: undo_mark_resp)
    }

    # A delete is an internal operation structurally similar to a store - it
    # mutates the tuplespace by removing a marked tuple from the tuplespace.
    # This is phase two of the 2PC protocol.

    subclass delete_req of msg_t = {
        field src : node
        field tpl : tuple

        action handle(self: node, ^msg: delete_req)
    }

    subclass delete_resp of msg_t = {
        field src : node
        field tpl : tuple
        field idem : bool # Was this an idempotent delete (ie. did this tuple not exist?)

        action handle(self: node, ^msg: delete_resp)
    }

    instance net : tcp.net(msg_t)

}

###############################################################################


process server(self: node) = {

    export action read(tpl: tuple_template) returns (ret: option[tuple])

    export action insert(tpl: tuple)
    export action remove(tpl: tuple_template)

    # Since insert and remove are "blocking" operations, have a separate
    # callback to be invoked when the operation succeeds.
    import action insert_resp(ret: tuple)
    import action remove_resp(ret: option[tuple])


    # For the current in-flight operation, how many have we heard back from?  
    #
    # TODO: at the moment, our commit point is when we've heard from
    # all nodes, though when we thread the manager in we'll need to make
    # sure that this then relates to specifically the nodes in the current
    # view.
    instance acks : ordered_set(node)
    var ack_count : nat # explicit counter since ordered_set doesn't expose a size()

    # The backing store for the node's tuples.
    instance tuples : tablet

    implementation {

        # The overlay network for servers to service client-level operations.
        instance sock : net.socket

        implement insert {
            var msg: store_req;
            msg.src := self;
            msg.tpl := tpl;
            broadcast(msg);
        }

        implement remove {
            var materialised : option[tuple] := tuples.get_match(tpl);
            if materialised.is_empty {
                # If there's nothing we can extract that matches the template,
                # then NACK back to the client.
                remove_resp(materialised);
            } else {
                # Go ahead and mark the tuples to be removed.
                var msg: mark_req;
                msg.src := self;
                msg.tpl := materialised.contents;
                broadcast(msg);
            }
        }

        implement read {
            ret := tuples.get_match(tpl);
        }

        #######################################################################

        # Network helpers (c/o Ken's HW6 starter)

        implement sock.recv(src:tcp.endpoint, msg:msg_t) {
            debug "server sock.recv" with server = self, msg = msg;
            msg.handle(self);
        }

        action unicast(outgoing : msg_t, dst_id : node) = {
            debug "send" with server = self, msg = outgoing, dst = dst_id;
            sock.send(server(dst_id).sock.id,outgoing);
        }

        action broadcast(outgoing : msg_t) = {
            for it, dst in node.iter {
                unicast(outgoing, dst);
            }
        }

        #######################################################################

        # Tuple writing

        implement store_req.handle {
            require inserting(msg.src);
            debug "store_req.handle" with self=self, msg=msg;

            # 1) Perform the local write.
            var already_present := tuples.add_tuple(msg.tpl);            

            # 2) Reply with a `store_resp`.
            var resp: store_resp;
            resp.idem := already_present;
            resp.src := self;
            resp.tpl := msg.tpl;
            unicast(resp, msg.src);

        }

        implement store_resp.handle {
            require inserting(self);

            debug "store_resp.handle" with self=self, msg=msg, ack_count=ack_count;
            acks.insert(msg.src);
            ack_count := ack_count + 1;

            if ack_count = cast(node.max) + 1 {
                # TODO: Have we received a resp from all nodes?
                # If so, commit.
                acks.erase(acks.begin(), acks.end());
                ack_count := 0;

                # 3) Ghost action to remember what we've stored.
                stored(self, msg.tpl) := true;

                insert_resp(msg.tpl);
            }
        }

        # Tuple marking

        implement mark_req.handle {
            debug "mark_req.handle" with self=self, msg=msg;
            # TODO: 1) Perform the local mark.
            var old_marked_state := tuples.mark(msg.tpl, msg.src);

            # 2) Reply with a `mark_resp`.
            var resp: mark_resp;
            resp.src := self;
            resp.tpl := msg.tpl;
            resp.ok := ~old_marked_state;
            unicast(resp, msg.src);
        }

        implement mark_resp.handle {
            debug "mark_resp.handle" with self=self, msg=msg;

            # If the marking failed, we need to roll back all nodes
            # that we did get ACKs from.
            if ~msg.ok {
                var msg2 : undo_mark_req;
                msg2.src := self;
                msg2.tpl := msg.tpl;

                #                for it, dst in acks.iter {
                #    unicast(msg2, dst);
                #}
            }

            # Have we received a resp from all nodes?
            # If so, finish the first phase of the commit protocol
            # and then progress to the actual deletion.
            acks.insert(msg.src);
            ack_count := ack_count + 1;

            if ack_count = cast(node.max) + 1 {
                acks.erase(acks.begin(), acks.end());
                ack_count := 0;

                var msg2 : delete_req;
                msg2.src := self;
                msg2.tpl := msg.tpl;
                broadcast(msg2);
            }
        }

        # Mark Rollback

        implement undo_mark_req.handle {
            debug "undo_mark_req.handle" with self=self, msg=msg;

            # 1) Perform the local unmark.
            tuples.unmark(msg.src);

            # 2) Reply with a `undo_mark_resp`.
            var resp: undo_mark_resp;
            resp.src := self;
            resp.tpl := msg.tpl;
            resp.ok := true; #TODO: I think this is ok.
            unicast(resp, msg.src);
        }

        implement undo_mark_resp.handle {
            debug "undo_mark_resp.handle" with self=self, msg=msg;
            # Have we received a resp from all the nodes who previously
            # ACKed to us?
            # If so, we can tell the client to retry.

            var it := node.iter.create(msg.src);
            assert it ~= acks.end();

            acks.erase(it, it.next);
            ack_count := ack_count - 1;
            if ack_count = 0 {
                acks.erase(acks.begin(), acks.end());
           
                remove_resp(option[tuple].empty);
            }
        }
        # Tuple deletion

        implement delete_req.handle {
            debug "delete_req.handle" with self=self, msg=msg;
            # TODO: 1) Perform the local delete, ensuring that we have
            # marked the tuple for deletion.

            # 2) Reply with a `delete_resp`.
            var resp: delete_resp;
            resp.idem := false; # TODO
            resp.src := self;
            resp.tpl := msg.tpl;
            unicast(resp, msg.src);

            stored(self, msg.tpl) := false;
        }

        implement delete_resp.handle {
            debug "delete_resp.handle" with self=self, msg=msg;

            # TODO: Have we received a successful resp from all nodes?
            acks.insert(msg.src);
            ack_count := ack_count + 1;

            if ack_count = cast(node.max) + 1 {
                acks.erase(acks.begin(), acks.end());
                ack_count := 0;

                remove_resp(option[tuple].just(msg.tpl));
            }
        }
    }

    ###########################################################################

    specification {
        common {
            # When this is called, we should consider the tuple committed.
            action store_committed(t: tuple)

            # When this is called, we should consider the tuple extracted.
            action remove_committed(t: tuple)

            # Is T stored in N's tablet?
            relation stored(N: node, T: tuple)

            # Does the supplied client have a request in flight?  (We do this
            # to serialise requests on a given node, since we assume that all
            # nodes are themselves executing sequentially.)
            relation client_operating(N: node)

            relation inserting(N: node)
            relation removing(N: node)

            invariant client_operating(N) <-> inserting(N) | removing(N)

            after init {
                stored(N,T) := false;
                client_operating(N) := false;
                inserting(N) := false;
                removing(N) := false;
            }

            after read(self: node, tpl: tuple_template) returns (ret: option[tuple]) {
                debug "after read" with ret=ret;
                # TODO: I think this is wrong: how does this behave with
                # concurrent removes?
                if ~ret.is_empty {
                    # If we return anything, it must be present somewhere in
                    # the tuplespace.  It's ok if it's not stored everywhere,
                    # since a read may be concurrent with an insert or removal.
                    require(exists N. stored(N, ret.contents));
                }
            }

            after insert_resp(self: node, ret: tuple) {
                require inserting(self);
                inserting(self) := false;

                # TODO

                call unblock_client(self);
            }

            after remove_resp(self: node, ret: option[tuple]) {
                require removing(self);
                removing(self) := false;

                # TODO: I think this is slightly wrong: how does this behave
                # with concurrent inserts of the same tuple?
                if ~ret.is_empty {
                    # If we return anything, it must have been extracted
                    # completely from the tuplespace.  
                    require(~stored(N, ret.contents));
                }

                call unblock_client(self);
            }
        }

        # Ensure sequential operations on a given node.
        # (TODO: maybe this is better moved into a `client` isolate?)
        before read {
            require ~client_operating(self);
        }
        before insert {
            require ~client_operating(self);
            require tpl.end > 0; # Don't try to insert the empty tuple.
            client_operating(self) := true;
            inserting(self) := true;
        }
        before remove {
            require ~client_operating(self);
            client_operating(self) := true;
            removing(self) := true;
        }

        # Mimic a blocking operation on the client's side returning.
        action unblock_client = {
            debug "unblock_client" with self=self;
            require client_operating(self);
            client_operating(self) := false;
        }

    }
}

axiom forall H0,H1. ~(server(H0).sock.id = server(H1).sock.id & H0 ~= H1)
attribute method=bmc[10]
