#lang ivy1.8
# A tuplespace server makes up a part of the replicated system.  It can consume
# API actions from an embedded client or from other server_ids.

include collections
include order
include network
include timeout

include manager
include tablet
include utils


###############################################################################

type active_state_t = {
    unjoined, # Nascent server, ready to join
    joining,  # In the process of getting tuplespaces replicated over
    live,     # Able to accept requests
    killed    # Failure detector downed it
}

type parked_state_t = {
    unparked, # Execution can proceed normally
    parking,  # Manager has asked us to park; we are waiting on a concurrent op to complete
    parked    # We will not service operations until the manager unparks us
}

global {
    instantiate server_rpc
    instantiate manager_rpc

    instance net : tcp.net(msg_t)
    instance man_net : tcp.net(man_msg_t)
}

instantiate manager_mod

process server(self: server_id) = {

    export action join

    export action read(tpl: tuple_template) returns (ret: option[tuple])

    export action insert(tpl: tuple)
    export action remove(tpl: tuple_template)

    # Since insert and remove are "blocking" operations, have a separate
    # callback to be invoked when the operation succeeds.
    import action insert_resp(ret: tuple)
    import action remove_resp(ret: option[tuple])

    import action park_commit
    import action node_unparked

    ##########################################################################

    common {
    }

    ##########################################################################

    # State variables for failover and replication


    # Has this node joined the tuplespace?
    var server_state : active_state_t

    # Who parked us?
    var parker : server_id

    # If we are chosen to replicate our tuples, to whom should that be?
    var replicate_to : server_id

    # Are we accepting operations, draining the operation we have in flight,
    # or blocking at the request of another node?
    var parked_state : parked_state_t

    # Has our client issued an asynchronous operation (removal, insertion)
    # that has not yet been committed?
    var op_inflight : bool

    # The servers that the node believes is part of the tuplespace.
    instance view: ordered_set(server_id)
    var view_number : nat

    # For the current in-flight operation, how many have we heard back from?
    #
    # TODO: at the moment, our commit point is when we've heard from all nodes
    # in the view, though when we thread the manager in we'll need to make sure
    # that this then relates to specifically the server_ids in the current
    # view.
    instance acks : ordered_set(server_id)

    # If we have requested a parking operation, how many ACKs back have we gotten?
    # (This is a separate state variable from `acks` as we might be in the middle of
    # an operation where we wait for ACKs to come back, and so we don't want to
    # interleave those.
    instance parked_acks : ordered_set(server_id)

    ##########################################################################

    # Are we undoing some marking operations?
    var rolling_back : bool 

    # Have we been informed that our services are no longer required?
    var should_die : bool

    # The backing store for the server_id's tuples.
    instance tuples : tablet

    implementation {

        # The overlay network for servers to service client-level operations.
        instance sock : net.socket

        # The overlay network for server <-> manager interaction.

        instance man_sock : man_net.socket

        after init {
            # TODO: the manager will give us the initial view, which could very
            # well be [0,1,2].  Maybe it's easier for the initial view to be
            # coordinated manually between the servers and the manager?
            view.insert(0);
            view.insert(1);
            view.insert(2);

            server_state := live if view.member(self) else unjoined;

            parked_state := unparked;
        }

        implement insert {
            if server_state = live {
                op_inflight := true;
                var msg: store_req;
                msg.src := self;
                msg.tpl := tpl;

                broadcast(msg);
            }
        }

        implement remove {
            if server_state = live {
                op_inflight := true;

                # To avoid racing on an already-marked tuple, set ignore_marked
                # to true.
                var materialised : option[tuple] := tuples.get_match(tpl, true);
                if materialised.is_empty {
                    # If there's nothing we can extract that matches the template,
                    # then NACK back to the client.
                    remove_resp(materialised);
                } else {
                    # Go ahead and mark the tuples to be removed.
                    var msg: mark_req;
                    msg.src := self;
                    msg.tpl := materialised.contents;

                    broadcast(msg);
                }
            }
        }

        implement read {
            # Since our commit point for removals is after marking, we are okay
            # to still concurrently read marked tuples, so set ignore_marked to
            # false.
            if server_state = live {
                ret := tuples.get_match(tpl, false);
            }
        }

        #######################################################################

        # Network helpers (c/o Ken's HW6 starter)

        implement sock.recv(src:tcp.endpoint, msg:msg_t) {
            msg.handle(self);
        }

        action unicast(outgoing : msg_t, dst_id : server_id) = {
            debug "send" with server = self, msg = outgoing, dst = dst_id;
            sock.send(server(dst_id).sock.id,outgoing);
        }

        action broadcast(outgoing: msg_t) = {
            var it := view.begin();
            var e := view.end();
            while it ~= e {
                unicast(outgoing, it.value());
                it := view.next(it);
            }
        }


        #######################################################################

        # Tuple writing

        implement store_req.handle(msg: store_req) {
            if server_state = live {
                debug "store_req.handle" with self=self, msg=msg;

                # This specification says that if we are known by the ghost state
                # to be inserting, it better be the case that we aren't in the
                # originator's view (ie. they committed without us.)
                require ~server(msg.src).view.member(self) | inserting(msg.src);

                # 1 Perform the local write.
                var already_present := tuples.add_tuple(msg.tpl);

                stored(self, msg.tpl) := true; # Ghost action to remember what we've stored.

                # 2 Reply with a `store_resp`.
                var resp: store_resp;
                resp.idem := already_present;
                resp.src := self;
                resp.tpl := msg.tpl;
                unicast(resp, msg.src);
            }
        }

        implement store_resp.handle(msg: store_resp) {
            if server_state = live {
                debug "store_resp.handle" with self=self, msg=msg;

                # This specification says that if we are known by the ghost state
                # to be inserting, it better be the case that we aren't in the
                # originator's view (ie. they committed without us.)
                require server(self).view.member(msg.src) | inserting(self);

                acks.insert(msg.src);

                if commit_reached {
                    # Have we received a resp from all server_ids?  If so, commit.
                    acks.erase(acks.begin(), acks.end());

                    insert_resp(msg.tpl);
                }
            }
        }

        # Tuple marking

        implement mark_req.handle(msg: mark_req) {
            if server_state = live {
                debug "mark_req.handle" with self=self, msg=msg;
                # 1 Perform the local mark.
                var ok := tuples.mark(msg.tpl, msg.src);

                # 2 Reply with a `mark_resp`.
                var resp: mark_resp;
                resp.src := self;
                resp.tpl := msg.tpl;
                resp.ok := ok;
                unicast(resp, msg.src);
            }
        }

        implement mark_resp.handle(msg: mark_resp) {
            if server_state = live {
                debug "mark_resp.handle" with self=self, msg=msg, rb=rolling_back;
                if ~rolling_back {

                    # If the marking failed, we need to roll back server_ids.
                    if ~msg.ok {
                        debug "rolling back mark" with self=self, tpl=msg.tpl;
                        rolling_back := true;
                        acks.erase(acks.begin(), acks.end());

                        var msg2 : undo_mark_req;
                        msg2.src := self;
                        msg2.tpl := msg.tpl;

                        broadcast(msg2);
                    } else {
                        # Have we received a successful resp from all server_ids?
                        # If so, finish the first phase of the commit protocol
                        # and then progress to the actual deletion.
                        acks.insert(msg.src);

                        if commit_reached {
                            #if ack_count = cast(server_id.max) + 1 
                            debug "Marking complete" with self=self, tpl=msg.tpl;
                            acks.erase(acks.begin(), acks.end());

                            var msg2 : delete_req;
                            msg2.src := self;
                            msg2.tpl := msg.tpl;
                            broadcast(msg2);
                        }
                    }
                }
            }
        }

        # Mark Rollback

        implement undo_mark_req.handle(msg: undo_mark_req) {
            if server_state = live {
                debug "undo_mark_req.handle" with self=self, msg=msg;

                # 1 Perform the local unmark.
                var ok := tuples.unmark(msg.src, msg.tpl);

                # 2 Reply with a `undo_mark_resp`.
                var resp: undo_mark_resp;
                resp.src := self;
                resp.tpl := msg.tpl;
                resp.ok := ok;
                unicast(resp, msg.src);
            }
        }

        implement undo_mark_resp.handle(msg: undo_mark_resp) {
            if server_state = live {
                debug "undo_mark_resp.handle" with self=self, msg=msg;

                if rolling_back {
                    acks.insert(msg.src);

                    # Have all the server_ids responded back saying they've unmarked?
                    # If so, we can tell the client to retry.
                    if commit_reached {
                        debug "Marking complete" with self=self, tpl=msg.tpl;
                        acks.erase(acks.begin(), acks.end());

                        var msg2 : delete_req;
                        msg2.src := self;
                        msg2.tpl := msg.tpl;
                        broadcast(msg2);
                    }
                }
            }
        }

        # New View Handling

        implement view_msg.handle(msg: view_msg) {
            # no active check here as we want ALL servers to see views
            debug "view_msg.handle" with self=self, msg=msg;
            view_number := msg.view;

            var b := view.begin();
            var e := view.end();
            view.erase(b, e);

            # put current servers in view
            var in_view : bool := false;
            for it, sv in msg.cur_servers {
                if sv = self {
                    in_view := true;
                }
                view.insert(sv);
            }
            if ~in_view & server_state = live {
                if op_inflight {
                    should_die := true;
                } else {
                    server_state := killed;
                }
            }
            debug "server_state check" with server=self, server_state=server_state;
        }

        # Tuple deletion

        implement delete_req.handle(msg: delete_req) {
            if server_state = live {
                debug "delete_req.handle" with self=self, msg=msg;
                # 1 Perform the local delete, ensuring that we have marked the
                # tuple for deletion.
                var ok := tuples.sweep(msg.tpl, msg.src);
                stored(self, msg.tpl) := false;

                # 2 Reply with a `delete_resp`.
                var resp: delete_resp;
                resp.idem := false; # TODO
                resp.src := self;
                resp.tpl := msg.tpl;
                unicast(resp, msg.src);
            }
        }

        implement delete_resp.handle(msg: delete_resp) {
            if server_state = live {
                debug "delete_resp.handle" with self=self, msg=msg;

                if ~rolling_back {
                    var it := server_id.iter.create(msg.src);
                    assert it ~= acks.end();

                    acks.insert(msg.src);

                    # Have we received a successful resp from all server_ids?  If so, return
                    # the tuple to the client and unblock them.
                    if commit_reached {
                        acks.erase(acks.begin(), acks.end());

                        remove_resp(option[tuple].just(msg.tpl));
                    }
                }
            }
        }



        ###### Server joining

        implement join {
            server_state := joining;

            var msg : join_msg_t;
            msg.src := self;
            man_sock.send(manager(0).sock.id,msg);
        }

        implement bootstrap_msg_t.handle(msg: bootstrap_msg_t) {
            debug "bootstrap_msg_t.handle" with self=self, msg=msg;

            # Remember who we are going to bootstrap, for after everyone is
            # parked.
            replicate_to := msg.new_server;

            # Broadcast a park request to all the nodes.  Once they come back,
            # we can proceed with the parked operation (currently: the parked
            # operation we have is replication, but in principle there could be
            # others as well.)
            var msg: park_req;
            msg.src := self;
            broadcast(msg);

        }

        implement park_req.handle(msg: park_req) {
            debug "park_req.handle" with self=self, parker=msg.src, inf=op_inflight;
            # If we don't have an outstanding operation, just park ourselves
            # immediately.  Otherwise, remember to do it after we commit next.
            if parked_state = unparked {
                parker := msg.src;
                if ~op_inflight {
                    parked_state := parked;

                    # send the response back, saying we'll not process anything more
                    # until we get the unpark.
                    var resp : park_resp;
                    resp.src := self;
                    unicast(resp, parker);
                } else {
                    # We will send the response back after the current operation
                    # commits.
                    parked_state := parking;
                }
            }
        }

        implement park_resp.handle(msg: park_resp) {
            debug "park_resp.handle" with self=self, parker=msg.src, inf=op_inflight;
            parked_acks.insert(msg.src);

            # Have all the server_ids responded back saying they've parked?
            # If so, we can tell the client to retry.
            if park_commit_reached {
                debug "Parking complete" with self=self;
                parked_acks.erase(parked_acks.begin(), parked_acks.end());
                park_commit;

                var r_req : replicate_req;
                r_req.src := self;
                r_req.tuples := tuples.get_all();

                unicast(r_req, replicate_to);

            }
        }

        implement replicate_req.handle(msg: replicate_req) {
            debug "replicate_req.handle" with self=self;

            var i := msg.tuples.begin();
            var e := msg.tuples.end();

            while i ~= e {
                var tpl := msg.tuples.get(i);
                var ignored := tuples.add_tuple(tpl);
                i := i + 1;
            }

            server_state := live;

            var resp : replicate_resp;
            unicast(resp, msg.src);
        }

        implement replicate_resp.handle(msg: replicate_resp) {
            debug "replicate_resp.handle" with self=self;

            #TODO: replicate
            var resp : unpark;
            resp.src := self;
            broadcast(resp);
        }


        implement unpark.handle(msg: unpark) {
            debug "unpark.handle" with self=self;
            if parked_state = parked & msg.src = parker {
                parked_state := unparked;
                node_unparked;
            }
        }

        # The commit point for inserts and removals is once we've heard back
        # from all servers in our view.  If we've reached the commit point,
        # then the set of nodes we've heard back from needs to be an improper
        # superset of the current view.  If no view changes have happened,
        # then the sets will be equal. However, it is possible that dead nodes
        # that we've gotten ACKs from might be removed from the view.
        action commit_reached returns (ret: bool) = {
            var vi := view.begin();
            var ve := view.end();

            ret := true;
            while ret & vi ~= ve {
                var val := vi.value();
                if ~acks.member(val) {
                    ret := false; 
                }

                vi := view.next(vi);
            }
        }
        # This code duplication is most unfortunate except that I can't typedef
        # ordered_set(server_id) to something without a `(` in it, so the parser
        # gets confused when it tries to parse the argument list to the action.
        action park_commit_reached returns (ret: bool) = {
            var vi := view.begin();
            var ve := view.end();

            ret := true;
            while ret & vi ~= ve {
                var val := vi.value();
                if ~parked_acks.member(val) {
                    ret := false; 
                }

                vi := view.next(vi);
            }
        }
    }

    ###########################################################################

    specification {
        common {
            # When this is called, we should consider the tuple committed.
            action store_committed(t: tuple)

            # When this is called, we should consider the tuple extracted.
            action remove_committed(t: tuple)

            # Is T stored in N's tablet?
            relation stored(N: server_id, T: tuple)

            # Does the supplied client have a request in flight?  (We do this
            # to serialise requests on a given server_id, since we assume that all
            # server_ids are themselves executing sequentially.)
            relation client_operating(N: server_id)

            relation inserting(N: server_id)
            relation removing(N: server_id)

            # Only nodes that believe themselves to be part of the tuplespace
            # should ever be issuing client requests.
            # TODO
            #invariant (server_state(N) ~= live) -> ~client_operating(N)

            # The only blocking operations we allow are inserts and removes,
            # and we can only be doing one at a time.
            invariant op_inflight(N) <-> client_operating(N)
            invariant client_operating(N) -> (parked_state(N) ~= parked)
            invariant client_operating(N) <-> inserting(N) | removing(N)
            invariant inserting(N) -> ~removing(N)
            invariant removing(N) -> ~inserting(N)

            # We should only be rolling back if we're in the process of removing
            # a tuple.
            invariant rolling_back(N) -> removing(N)


            after init {
                stored(N,T) := false;
                client_operating(N) := false;
                inserting(N) := false;
                removing(N) := false;
                rolling_back(N) := false;
            }

            after read(self: server_id, tpl: tuple_template) returns (ret: option[tuple]) {
                debug "after read" with ret=ret;
                if ~ret.is_empty {
                    # If we return anything, it must be present somewhere in
                    # the tuplespace.  It's ok if it's not stored everywhere,
                    # since a read may be concurrent with an insert or removal.
                    require(exists N. server(N).server_state = live -> stored(N, ret.contents));
                }
            }

            after insert_resp(self: server_id, ret: tuple) {
                require inserting(self);
                inserting(self) := false;
                op_inflight(self) := false;

                # Not sure that this is the best way to write this
                # specification, but the intention is that in the presence of a
                # concurrent insert and remove, either the insert completed on
                # all nodes (serialising the two concurrent operations as
                # remove -> insert), or it failed completely (serialising as
                # insert -> remove).
                require (server(N).server_state = live -> stored(N, ret)) |
                (server(N).server_state = live -> ~stored(N, ret));

                call unblock_client(self);
            }

            after remove_resp(self: server_id, ret: option[tuple]) {
                require removing(self);
                removing(self) := false;
                op_inflight(self) := false;

                if ~ret.is_empty {
                    # Similarly to the above: If we return anything, it must
                    # have been extracted completely from the tuplespace, or,
                    # if concurrent with an insert, that remove must not have
                    # happened on any node.  The lhs indicates that the remove
                    # would be serialised second, and the rhs that the insert
                    # was.
                    require (server(N).server_state = live -> stored(N, ret.contents)) |
                    (server(N).server_state = live -> ~stored(N, ret.contents));
                }

                call unblock_client(self);
            }
        }

        before replicate_req.handle {
            #require view(server(N).parked_state = parked;
            require server_state = joining;
        }

        before join {
            require server_state = unjoined;
        }


        # Ensure sequential operations on a given server_id.
        # (TODO: maybe this is better moved into a `client` isolate?)
        before read {
            require server_state = live & ~should_die;
            require parked_state = unparked;
            require tpl.end > 0; # Don't try to read the empty tuple.
            require ~client_operating(self);
        }
        before insert {
            require server_state = live & ~should_die;
            # TODO: we will later enqueue operations for a parked server
            # to be done after unparking
            require parked_state = unparked;
            require ~client_operating(self);
            require tpl.end > 0; # Don't try to insert the empty tuple.
            client_operating(self) := true;
            inserting(self) := true;
        }
        before remove {
            require server_state = live;
            # TODO: we will later enqueue operations for a parked server
            # to be done after unparking
            require parked_state = unparked;
            require ~client_operating(self);
            require tpl.end > 0; # Don't try to remove the empty tuple.
            client_operating(self) := true;
            removing(self) := true;
        }

        # Mimic a blocking operation on the client's side returning.
        action unblock_client = {
            debug "unblock_client" with self=self;
            require client_operating(self);

            # Now that our operation is done: if we promised to park, do
            # it!
            if parked_state = parking {
                parked_state := parked;

                # send the response back, saying we'll not process anything more
                # until we get the unpark.
                var resp : park_resp;
                resp.src := self;
                unicast(resp, parker);
            }

            # Similarly, if we were told to die, this is our final operation.
            if should_die {
                server_state := killed; 
            }

            client_operating(self) := false;
        }

    }
}


axiom forall H0,H1. ~(server(H0).sock.id = server(H1).sock.id & H0 ~= H1)
attribute method=bmc[10]

# To mimic the "predomently-read only" workload, reduce the number of removes
# and increase the number of reads.  (The system works fine without doing this
# but in my head this creates a more plausable-looking execution trace, since
# with all actions being of equal probability, most random reads end up being
# NACKs...)
attribute server.read.weight = "3"
attribute server.remove.weight = "0.33"

# Topology changes should be very infrequent just so we spread the events out
# through the test run.
attribute server.join.weight = "0.01"

