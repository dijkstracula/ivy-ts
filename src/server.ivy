#lang ivy1.8

# A tuplespace server makes up a part of the replicated system.  It can consume
# API actions from an embedded client or from other nodes.

include collections
include order
include network

include tablet
include ts_types

global {

    # All server messages are internal to the tuplespace implementation;
    # clients do not directly send these messages but simply "call into" the
    # tuplespace with exported actions.

    # TODO: It would be nice to be able to namespace these messages under
    # `server` to imply a certain information hiding, but I get errors about
    # dependency cycles...
    class tuple_msg_t = {
        # The operation to take when a node receives a particular message.
        action handle(self: node, ^msg:tuple_msg_t)

        # If the message is part of a larger protocol (such as a req/resp
        # pair), produce the message that should be returned to the sending
        # node.  Produces an empty return value if no such message need be
        # sent.  (The caller may still need to fill in certain fields.)
        # TODO: is this actually useful?  I'm not so sure anymore.
        #action next(self: node, ^msg:tuple_msg_t) returns (ret: option[tuple_msg_t])
    }


    # A store is an internal operation mutates the tuplespace by adding a tuple
    # to all nodes in the tuplespace.  For details, see doc/operations.md .

    subclass tuple_store_req of tuple_msg_t = {
        field src : node
        field tpl : tuple

        action handle(self: node, ^msg: tuple_store_req)
    }

    subclass tuple_store_resp of tuple_msg_t = {
        field src : node
        field idem : bool # Was this an idempotent write (ie. did this tuple already exist?)

        action handle(self: node, ^msg: tuple_store_resp)
    }

    # A mark indicates that a remote server wishes to atomically remove a value
    # from the tuplestore.

    subclass tuple_mark_req of tuple_msg_t = {
        field src : node
        field tpl : tuple

        action handle(self: node, ^msg: tuple_mark_req)
    }

    subclass tuple_mark_resp of tuple_msg_t = {
        field src : node
        field ok : bool # Did we mark this successfully?

        action handle(self: node, ^msg: tuple_mark_resp)
    }


    # A delete is an internal operation structurally similar to a store - it mutates
    # the tuplespace by removing a marked tuple from the tuplespace.

    subclass tuple_delete_req of tuple_msg_t = {
        field src : node
        field tpl : tuple

        action handle(self: node, ^msg: tuple_delete_req)
    }

    subclass tuple_delete_resp of tuple_msg_t = {
        field src : node
        field idem : bool # Was this an idempotent delete (ie. did this tuple not exist?)

        action handle(self: node, ^msg: tuple_delete_resp)
    }

    instance tuple_net : tcp.net(tuple_msg_t)
}


###############################################################################


process server(self: node) = {

    export action read(tpl: tuple_template) returns (ret: option[tuple])

    export action insert(tpl: tuple)
    export action remove(tpl: tuple_template)

    # Since insert and remove are "blocking" operations, have a separate
    # callback to be invoked when the operation succeeds.
    import action insert_resp(ret: tuple)
    import action remove_resp(ret: option[tuple])

    implementation {

        # The overlay network for servers to service client-level operations.
        instance sock : tuple_net.socket

        # The backing store for the node's tuples.
        instance tuples : tablet

        # The in-flight operations and whom we've gotten ACKs back from.
        # TODO

        implement insert {
            var msg: tuple_store_req;
            msg.src := self;
            msg.tpl := tpl;
            broadcast(msg);
        }

        implement remove {
            var materialised : option[tuple] := read(tpl);
            if materialised.is_empty {
                # If there's nothing we can extract that matches the template,
                # then NACK back to the client.
                remove_resp(materialised);
            } else {
                # Go ahead and mark the tuples to be removed.
                var msg: tuple_mark_req;
                msg.src := self;
                msg.tpl := materialised.contents;
                broadcast(msg);
            }
        }

        implement read {
            # TODO: If the tuple's components are all defined, then we can decay
            # into an exact search.  

            # Otherwise, do a wildcarded search.
            ret := tuples.get_match(tpl);
        }

        #######################################################################

        # Network helpers (c/o Ken's HW6 starter)

        implement sock.recv(src:tcp.endpoint, msg:tuple_msg_t) {
            debug "server sock.recv" with server = self, msg = msg;
            msg.handle(self);
        }

        action unicast(outgoing : tuple_msg_t, dst_id : node) = {
            debug "send" with server = self, msg = outgoing, dst = dst_id;
            sock.send(server(dst_id).sock.id,outgoing);
        }

        action broadcast(outgoing : tuple_msg_t) = {
            for it, dst in node.iter {
                unicast(outgoing, dst);
            }
        }

        #######################################################################

        # Tuple writing

        implement tuple_store_req.handle {
            debug "tuple_store_req.handle" with self=self, msg=msg;
            # 1) Perform the local write.
            var already_present := tuples.add_tuple(msg.tpl);            

            # 2) Reply with a `tuple_store_resp`.
            var resp: tuple_store_resp;
            resp.idem := already_present;
            resp.src := self;
            unicast(resp, msg.src);

            # 3) Ghost action to remember what we've stored.
            ghost_stored(self, msg.tpl) := true;
        }

        implement tuple_store_resp.handle {
            debug "tuple_store_resp.handle" with self=self, msg=msg;
            # TODO: Have we received a resp from all nodes?
            # If so, commit.
        }

        # Tuple marking

        implement tuple_mark_req.handle {
            debug "tuple_mark_req.handle" with self=self, msg=msg;
            # TODO: 1) Perform the local write.

            # 2) Reply with a `tuple_mark_resp`.
            var resp: tuple_mark_resp;
            resp.ok := false; # TODO
            resp.src := self;
            unicast(resp, msg.src);
        }

        implement tuple_mark_resp.handle {
            debug "tuple_mark_resp.handle" with self=self, msg=msg;
            # TODO: Have we received a resp from all nodes?
            # If so, commit.
        }
       
        # Tuple deletion

        implement tuple_delete_req.handle {
            debug "tuple_delete_req.handle" with self=self, msg=msg;
            # TODO: 1) Perform the local delete, ensuring that we have
            # marked the tuple for deletion.

            # 2) Reply with a `tuple_delete_resp`.
            var resp: tuple_delete_resp;
            resp.idem := false; # TODO
            resp.src := self;
            unicast(resp, msg.src);

            ghost_stored(self, msg.tpl) := false;
        }

        implement tuple_delete_resp.handle {
            debug "tuple_delete_resp.handle" with self=self, msg=msg;
            # TODO: Have we received a resp from all nodes?
            # If so, commit.
        }
    }

    ###########################################################################

    common {
        specification {

            # When this is called, we should consider the tuple committed.
            action store_committed(t: tuple)

            # When this is called, we should consider the tuple extracted.
            action remove_committed(t: tuple)

            # Is the given tuple 
            relation ghost_stored(N: node, T: tuple)

            after init {
                ghost_stored(N,T) := false;
            }

            after read(self: node, tpl: tuple_template) returns (ret: option[tuple]) {
                # TODO: I think this is wrong: how does this behave with
                # concurrent removes?
                if ~ret.is_empty {
                    # If we return anything, it must be present in the tuplespace.
                    require(ghost_stored(N, ret.contents));
                }
            }

            after insert_resp(self: node, ret: tuple) {
                # Oh lordy, how does this behave with concurrent removals?
                ensure forall N. ghost_stored(N, ret);
            }

            after remove_resp(self: node, ret: option[tuple]) {
                # TODO: I think this is wrong: how does this behave with
                # concurrent inserts?
                if ~ret.is_empty {
                    # If we return anything, it must have been extracted from the tuplespace.
                    require(~ghost_stored(N, ret.contents));
                }
            }
        }
    }
}

