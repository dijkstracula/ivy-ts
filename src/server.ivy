#lang ivy1.8

# A tuplespace server makes up a part of the replicated system.  It can consume
# API actions from an embedded client or from other server_ids.

include collections
include order
include network
include timeout

include manager
include tablet
include utils

instantiate manager_mod

global {

    # All server messages are internal to the tuplespace implementation;
    # clients do not directly send these messages but simply "call into" the
    # tuplespace with exported actions.

    # TODO: It would be nice to be able to namespace these messages under
    # `server` to imply a certain information hiding, but I get errors about
    # dependency cycles...
    class msg_t = {
        # The operation to take when a server_id receives a particular message.
        action handle(self: server_id, ^msg:msg_t)

        # If the message is part of a larger protocol (such as a req/resp
        # pair), produce the message that should be returned to the sending
        # server_id.  Produces an empty return value if no such message need be
        # sent.  (The caller may still need to fill in certain fields.)
        # TODO: is this actually useful?  I'm not so sure anymore.
        # TODO: revisit the nathan/rpc_metaprotocol branch if there's time.
        # The `andThen` combinator might be what we want here.
        #action next(self: server_id, ^msg:msg_t) returns (ret: option[msg_t])
    }


    # A store is an internal operation mutates the tuplespace by adding a tuple
    # to all server_ids in the tuplespace.  For details, see doc/operations.md .

    subclass store_req of msg_t = {
        field src : server_id
        field tpl : tuple

        action handle(self: server_id, ^msg: store_req)
    }

    subclass store_resp of msg_t = {
        field src : server_id
        field tpl : tuple
        field idem : bool # Was this an idempotent write (ie. did this tuple already exist?)

        action handle(self: server_id, ^msg: store_resp)
    }

    # A mark indicates that a remote server wishes to atomically remove a value
    # from the tuplestore.  This is phase one of the 2PC protocol.

    subclass mark_req of msg_t = {
        field src : server_id
        field tpl : tuple

        action handle(self: server_id, ^msg: mark_req)
    }

    subclass mark_resp of msg_t = {
        field src : server_id
        field tpl : tuple
        field ok : bool # Did we mark this successfully?

        action handle(self: server_id, ^msg: mark_resp)
    }


    # A mark rollback indicates that a remote server, having previously marked
    # some tuples during the first phase of a remove operation, would like to
    # undo those markings.

    subclass undo_mark_req of msg_t = {
        field src : server_id
        field tpl : tuple

        action handle(self: server_id, ^msg: undo_mark_req)
    }

    subclass undo_mark_resp of msg_t = {
        field src : server_id
        field tpl : tuple
        field ok : bool # Did we mark this successfully?

        action handle(self: server_id, ^msg: undo_mark_resp)
    }

    # A delete is an internal operation structurally similar to a store - it
    # mutates the tuplespace by removing a marked tuple from the tuplespace.
    # This is phase two of the 2PC protocol.

    subclass delete_req of msg_t = {
        field src : server_id
        field tpl : tuple

        action handle(self: server_id, ^msg: delete_req)
    }

    subclass delete_resp of msg_t = {
        field src : server_id
        field tpl : tuple
        field idem : bool # Was this an idempotent delete (ie. did this tuple not exist?)

        action handle(self: server_id, ^msg: delete_resp)
    }

    # A replicate is an internal operation where a nascent node requests
    # from another node the contents of its tuplespace.

    subclass replicate_req of msg_t = {
        field src : server_id

        action handle(self: server_id, ^msg: replicate_req)
    }

    subclass replicate_resp of msg_t = {
        field is_active : bool # False if we sent this to a node that has parted
        field tuples : vector[tuple] # the full contents of the node's tablet

        action handle(self: server_id, ^msg: replicate_resp)
    }


    instance net : tcp.net(msg_t)
}

###############################################################################


process server(self: server_id) = {

    export action join

    export action read(tpl: tuple_template) returns (ret: option[tuple])

    export action insert(tpl: tuple)
    export action remove(tpl: tuple_template)

    # Since insert and remove are "blocking" operations, have a separate
    # callback to be invoked when the operation succeeds.
    import action insert_resp(ret: tuple)
    import action remove_resp(ret: option[tuple])

    ##########################################################################

    instantiate manager_rpc

    ##########################################################################

    # State variables for failover and replication


    # Has this node joined the tuplespace?
    var active : bool

    # Is this node in the process of faulting in its tuples from another 
    # node? 
    # TODO: Not sure exactly what the semantics should be here, yet: should it
    # be the case that we can be active but also replicating (in which case we
    # need to enqueue operations to do?)  I think this is true, but need to
    # meditate on that some more.
    var replicating : bool

    # The servers that the node believes is part of the tuplespace.
    instance view: ordered_set(server_id)

    # For the current in-flight operation, how many have we heard back from?
    #
    # TODO: at the moment, our commit point is when we've heard from all nodes
    # in the view, though when we thread the manager in we'll need to make sure
    # that this then relates to specifically the server_ids in the current
    # view.
    instance acks : ordered_set(server_id)

    # Are we undoing some marking operations?
    var rolling_back : bool 

    # The backing store for the server_id's tuples.
    instance tuples : tablet

    implementation {

        # The overlay network for servers to service client-level operations.
        instance sock : net.socket

        # The overlay network for server <-> manager interaction.
        instance man_net : tcp.net(man_msg_t)
        instance man_sock : man_net.socket

        after init {
            # TODO: the manager will give us the initial view, which could very
            # well be [0,1,2].  Maybe it's easier for the initial view to be
            # coordinated manually between the servers and the manager?
            view.insert(0);
            view.insert(1);
            view.insert(2);

            active := (view.member(self));
        }

        implement insert {
            var msg: store_req;
            msg.src := self;
            msg.tpl := tpl;

            broadcast(msg);
        }

        implement remove {
            # To avoid racing on an already-marked tuple, set ignore_marked
            # to true.
            var materialised : option[tuple] := tuples.get_match(tpl, true);
            if materialised.is_empty {
                # If there's nothing we can extract that matches the template,
                # then NACK back to the client.
                remove_resp(materialised);
            } else {
                # Go ahead and mark the tuples to be removed.

                var msg: mark_req;
                msg.src := self;
                msg.tpl := materialised.contents;

                broadcast(msg);
            }
        }

        implement read {
            # Since our commit point for removals is after marking, we are okay
            # to still concurrently read marked tuples, so set ignore_marked to
            # false.
            ret := tuples.get_match(tpl, false);
        }

        #######################################################################

        # Network helpers (c/o Ken's HW6 starter)

        implement sock.recv(src:tcp.endpoint, msg:msg_t) {
            msg.handle(self);
        }

        action unicast(outgoing : msg_t, dst_id : server_id) = {
            debug "send" with server = self, msg = outgoing, dst = dst_id;
            sock.send(server(dst_id).sock.id,outgoing);
        }

        action broadcast(outgoing: msg_t) = {
            var it := view.begin();
            var e := view.end();
            while it ~= e {
                unicast(outgoing, it.value());
                it := view.next(it);
            }
        }


        #######################################################################

        # Tuple writing

        implement store_req.handle {
            debug "store_req.handle" with self=self, msg=msg;
            require inserting(msg.src);

            # 1) Perform the local write.
            var already_present := tuples.add_tuple(msg.tpl);
            stored(self, msg.tpl) := true; # Ghost action to remember what we've stored.

            # 2) Reply with a `store_resp`.
            var resp: store_resp;
            resp.idem := already_present;
            resp.src := self;
            resp.tpl := msg.tpl;
            unicast(resp, msg.src);
        }

        implement store_resp.handle {
            debug "store_resp.handle" with self=self, msg=msg;
            require inserting(self);

            acks.insert(msg.src);

            if commit_reached {
                # Have we received a resp from all server_ids?  If so, commit.
                acks.erase(acks.begin(), acks.end());

                insert_resp(msg.tpl);
            }
        }

        # Tuple marking

        implement mark_req.handle {
            debug "mark_req.handle" with self=self, msg=msg;
            # 1) Perform the local mark.
            var ok := tuples.mark(msg.tpl, msg.src);

            # 2) Reply with a `mark_resp`.
            var resp: mark_resp;
            resp.src := self;
            resp.tpl := msg.tpl;
            resp.ok := ok;
            unicast(resp, msg.src);
        }
        implement mark_resp.handle {
            debug "mark_resp.handle" with self=self, msg=msg, rb=rolling_back;
            if ~rolling_back {

                # If the marking failed, we need to roll back server_ids.
                if ~msg.ok {
                    debug "rolling back mark" with self=self, tpl=msg.tpl;
                    rolling_back := true;
                    acks.erase(acks.begin(), acks.end());

                    var msg2 : undo_mark_req;
                    msg2.src := self;
                    msg2.tpl := msg.tpl;

                    broadcast(msg2);
                } else {
                    # Have we received a successful resp from all server_ids?
                    # If so, finish the first phase of the commit protocol
                    # and then progress to the actual deletion.
                    acks.insert(msg.src);

                    if commit_reached {
                        debug "Marking complete" with self=self, tpl=msg.tpl;
                        acks.erase(acks.begin(), acks.end());

                        var msg2 : delete_req;
                        msg2.src := self;
                        msg2.tpl := msg.tpl;
                        broadcast(msg2);
                    }
                }
            }
        }

        # Mark Rollback

        implement undo_mark_req.handle {
            debug "undo_mark_req.handle" with self=self, msg=msg;

            # 1) Perform the local unmark.
            var ok := tuples.unmark(msg.src, msg.tpl);

            # 2) Reply with a `undo_mark_resp`.
            var resp: undo_mark_resp;
            resp.src := self;
            resp.tpl := msg.tpl;
            resp.ok := ok;
            unicast(resp, msg.src);
        }

        implement undo_mark_resp.handle {
            debug "undo_mark_resp.handle" with self=self, msg=msg;

            if rolling_back {
                acks.insert(msg.src);

                # Have all the server_ids responded back saying they've unmarked?
                # If so, we can tell the client to retry.
                if commit_reached {
                    debug "Rollback complete" with self=self, tpl=msg.tpl;
                    rolling_back := false;
                    acks.erase(acks.begin(), acks.end());

                    remove_resp(option[tuple].empty);
                }
            }
        }

        # Tuple deletion

        implement delete_req.handle {
            debug "delete_req.handle" with self=self, msg=msg;
            # 1) Perform the local delete, ensuring that we have marked the
            # tuple for deletion.
            var ok := tuples.sweep(msg.tpl, msg.src);
            stored(self, msg.tpl) := false;

            # 2) Reply with a `delete_resp`.
            var resp: delete_resp;
            resp.idem := false; # TODO
            resp.src := self;
            resp.tpl := msg.tpl;
            unicast(resp, msg.src);
        }

        implement delete_resp.handle {
            debug "delete_resp.handle" with self=self, msg=msg;

            if ~rolling_back {
                var it := server_id.iter.create(msg.src);
                assert it ~= acks.end();

                acks.insert(msg.src);

                # Have we received a successful resp from all server_ids?  If so, return
                # the tuple to the client and unblock them.
                if commit_reached {
                    acks.erase(acks.begin(), acks.end());

                    remove_resp(option[tuple].just(msg.tpl));
                }
            }
        }

        implement replicate_req.handle {
            # 
        }

        implement replicate_resp.handle {

        }

        # The commit point for inserts and removals is once we've heard back
        # from all servers in our view.  If we've reached the commit point,
        # then the set of nodes we've heard back from needs to be an improper
        # superset of the current view.  (If no view changes have happened,
        # then the sets will be equal. However, it is possible that dead nodes
        # that we've gotten ACKs from might be removed from the view.)
        action commit_reached returns (ret: bool) = {
            var it := view.begin();
            var e := view.end();

            ret := true;
            while ret & it ~= e {
                var val := it.value();
                if ~acks.member(val) {
                    ret := false; 
                }

                it := view.next(it);
            }
        }

    }

    ###########################################################################

    specification {
        common {
            # When this is called, we should consider the tuple committed.
            action store_committed(t: tuple)

            # When this is called, we should consider the tuple extracted.
            action remove_committed(t: tuple)

            # Is T stored in N's tablet?
            relation stored(N: server_id, T: tuple)

            # Does the supplied client have a request in flight?  (We do this
            # to serialise requests on a given server_id, since we assume that all
            # server_ids are themselves executing sequentially.)
            relation client_operating(N: server_id)

            relation inserting(N: server_id)
            relation removing(N: server_id)

            # Only nodes that believe themselves to be part of the tuplespace
            # should ever be issuing client requests.
            #invariant ~active(N) -> ~client_operating(N)

            # The only blocking operations we allow are inserts and removes,
            # and we can only be doing one at a time.
            invariant client_operating(N) <-> inserting(N) | removing(N)
            invariant inserting(N) -> ~removing(N)
            invariant removing(N) -> ~inserting(N)

            # We should only be rolling back if we're in the process of removing
            # a tuple.
            invariant rolling_back(N) -> removing(N)


            after init {
                stored(N,T) := false;
                client_operating(N) := false;
                inserting(N) := false;
                removing(N) := false;
                rolling_back(N) := false;
            }

            after read(self: server_id, tpl: tuple_template) returns (ret: option[tuple]) {
                debug "after read" with ret=ret;
                if ~ret.is_empty {
                    # If we return anything, it must be present somewhere in
                    # the tuplespace.  It's ok if it's not stored everywhere,
                    # since a read may be concurrent with an insert or removal.
                    require(exists N. server(N).active -> stored(N, ret.contents));
                }
            }

            after insert_resp(self: server_id, ret: tuple) {
                require inserting(self);
                inserting(self) := false;

                # Not sure that this is the best way to write this
                # specification, but the intention is that in the presence of a
                # concurrent insert and remove, either the insert completed on
                # all nodes (serialising the two concurrent operations as
                # remove -> insert), or it failed completely (serialising as
                # insert -> remove).
                require (server(N).active -> stored(N, ret)) |
                        (server(N).active -> ~stored(N, ret));

                call unblock_client(self);
            }

            after remove_resp(self: server_id, ret: option[tuple]) {
                require removing(self);
                removing(self) := false;

                if ~ret.is_empty {
                    # Similarly to the above: If we return anything, it must
                    # have been extracted completely from the tuplespace, or,
                    # if concurrent with an insert, that remove must not have
                    # happened on any node.  The lhs indicates that the remove
                    # would be serialised second, and the rhs that the insert
                    # was.
                    require (server(N).active -> stored(N, ret.contents)) |
                            (server(N).active -> ~stored(N, ret.contents));
                    }

                call unblock_client(self);
            }
        }

        before join {
            require ~active;
        }

        # Ensure sequential operations on a given server_id.
        # (TODO: maybe this is better moved into a `client` isolate?)
        before read {
            require active;
            require tpl.end > 0; # Don't try to read the empty tuple.
            require ~client_operating(self);
        }
        before insert {
            require active;
            require ~client_operating(self);
            require tpl.end > 0; # Don't try to insert the empty tuple.
            client_operating(self) := true;
            inserting(self) := true;
        }
        before remove {
            require active;
            require ~client_operating(self);
            require tpl.end > 0; # Don't try to remove the empty tuple.
            client_operating(self) := true;
            removing(self) := true;
        }

        # Mimic a blocking operation on the client's side returning.
        action unblock_client = {
            debug "unblock_client" with self=self;
            require client_operating(self);
            client_operating(self) := false;
        }

    }
}


axiom forall H0,H1. ~(server(H0).sock.id = server(H1).sock.id & H0 ~= H1)
attribute method=bmc[10]

# To mimic the "predomently-read only" workload, reduce the number of removes
# and increase the number of reads.  (The system works fine without doing this
# but in my head this creates a more plausable-looking execution trace, since
# with all actions being of equal probability, most random reads end up being
# NACKs...)
attribute server.read.weight = "3"
attribute server.remove.weight = "0.33"

# Topology changes should be very infrequent just so we spread the events out
# through the test run.
attribute server.join.weight = "0.01"
attribute manager.is_down.weight = "0.01"
